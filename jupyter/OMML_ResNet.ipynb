{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeGe9uFY1oaF"
      },
      "source": [
        "#Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8_KTSWqGSMhf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.utils as vutils\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S7aXqCbOeFtl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomAdam(optim.Optimizer):\n",
        "    def __init__(self, params, lr=2e-4, betas=(0.5, 0.9), eps=1e-8):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
        "        super(CustomAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Update biased first moment estimate\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                # Update biased second moment estimate\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                # Bias correction\n",
        "                bias_correction = 1 - beta2 ** state['step']\n",
        "                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction)).add_(group['eps'])\n",
        "\n",
        "                # Update parameters\n",
        "                step_size = group['lr']\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYCFTlAJv7he",
        "outputId": "61158ed0-4202-405f-a516-48056a03bcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Generator Updates:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Dataloader:   0%|          | 0/782 [00:00<?, ?it/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_generator_updates = 1\n",
        "adam_beta1 = 0.5\n",
        "adam_beta2 = 0.9\n",
        "gradient_penalty_weight = 10\n",
        "lr_generator = 2e-5\n",
        "lr_discriminator = 2e-4\n",
        "beta_ema = 0.9999\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset = datasets.CIFAR10(root='./data', download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=128):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(z_dim, 128 * 4 * 4)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            ResBlock(128),\n",
        "            ResBlock(128),\n",
        "            ResBlock(128)\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm2d(128)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Add additional transposed convolution layers for upsampling\n",
        "        self.transposed_conv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.transposed_conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.transposed_conv3 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.linear(z)\n",
        "        x = x.view(-1, 128, 4, 4)\n",
        "        x = self.resblocks(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Upsample to 32x32\n",
        "        x = self.transposed_conv1(x)\n",
        "        x = self.transposed_conv2(x)\n",
        "        x = self.transposed_conv3(x)\n",
        "\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.resblocks = nn.Sequential(\n",
        "            ResBlock(3),\n",
        "            ResBlock(128),\n",
        "            ResBlock(128),\n",
        "            ResBlock(128)\n",
        "        )\n",
        "        self.linear = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resblocks(x)\n",
        "        x = x.mean(dim=(2, 3))  # Global average pooling\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Residual Block\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=128, stride=1):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "'''\n",
        "optimizer_generator = optim.Adam(generator.parameters(), lr=lr_generator, betas=(adam_beta1, adam_beta2))\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=lr_discriminator, betas=(adam_beta1, adam_beta2))\n",
        "'''\n",
        "optimizer_generator = CustomAdam(generator.parameters(), lr_generator, betas=(adam_beta1, adam_beta2))\n",
        "optimizer_discriminator = CustomAdam(discriminator.parameters(), lr=lr_discriminator,betas=(adam_beta1, adam_beta2))\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "loss_function = torch.nn.BCELoss()\n",
        "\n",
        "def compute_discriminator_loss(output_discriminator, all_samples_labels):\n",
        "    # print(output_discriminator)\n",
        "    # print(all_samples_labels)\n",
        "    return loss_function(output_discriminator, all_samples_labels)\n",
        "\n",
        "def compute_generator_loss(output_discriminator_generated, all_samples_labels):\n",
        "    return loss_function(output_discriminator_generated, all_samples_labels)\n",
        "\n",
        "def compute_gradient_penalty(real_data, fake_data, discriminator):\n",
        "    # Interpolation\n",
        "    alpha = torch.rand(real_data.size(0), 1, 1, 1).to(real_data.device)\n",
        "    print(alpha.shape)\n",
        "    print(real_data.shape)\n",
        "    print(fake_data.shape)\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    # Calculate discriminator scores\n",
        "    disc_interpolates = discriminator(interpolates)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones(disc_interpolates.size()).to(real_data.device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    # Compute gradient penalty\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gradient_penalty_weight\n",
        "\n",
        "    return gradient_penalty\n",
        "\n",
        "# Assuming ema_model_params is a list that stores the EMA parameters\n",
        "ema_model_params = [p.data.clone() for p in generator.parameters()]\n",
        "\n",
        "# Training loop\n",
        "for update in tqdm(range(num_generator_updates), desc=\"Generator Updates\"):\n",
        "\n",
        "    for real_data in tqdm(dataloader, desc=\"Dataloader\", leave=False):\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        # Generate fake data\n",
        "        fake_data = generator(torch.randn(batch_size, 128))  # Assuming z_dim is 128\n",
        "\n",
        "        # Compute WGAN loss with gradient penalty\n",
        "        loss_discriminator = compute_discriminator_loss(real_data[0], fake_data, discriminator)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # Train generator\n",
        "        generator.zero_grad()\n",
        "\n",
        "        # Generate fake data\n",
        "        fake_data = generator(torch.randn(batch_size, 128))  # Assuming z_dim is 128\n",
        "\n",
        "        # Compute generator loss\n",
        "        loss_generator = compute_generator_loss(fake_data, discriminator)\n",
        "        loss_generator.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        # Gradient penalty\n",
        "        # gradient_penalty = compute_gradient_penalty(real_data[0], fake_data, discriminator)\n",
        "        # gradient_penalty.backward(retain_graph=True)\n",
        "        # optimizer_discriminator.step()\n",
        "\n",
        "        # Update EMA parameters\n",
        "        # update_ema(generator.parameters(), beta_ema)\n",
        "\n",
        "    # Print losses or save images for evaluation\n",
        "    if update % 100 == 0:\n",
        "        print(f'Update: {update}, Generator Loss: {loss_generator.item()}, Discriminator Loss: {loss_discriminator.item()}')\n",
        "\n",
        "    if update % 500 == 0:\n",
        "        with torch.no_grad():\n",
        "            generator.eval()\n",
        "            fake_samples = generator(torch.randn(64, 128))  # Assuming z_dim is 128\n",
        "            vutils.save_image(fake_samples, f'generated_samples_{update}.png', normalize=True)\n",
        "\n",
        "# Evaluate the generator (generate samples)\n",
        "with torch.no_grad():\n",
        "    generator.eval()\n",
        "    fake_samples = generator(torch.randn(64, 128))  # Assuming z_dim is 128\n",
        "    vutils.save_image(fake_samples, 'generated_samples.png', normalize=True)\n",
        "\n",
        "# Save the trained models\n",
        "torch.save(generator.state_dict(), 'generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}